{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$env_openai_keys\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import dotenv_values \n",
    "\n",
    "# Load .env configuration\n",
    "dotenv_path = r\"c:\\Users\\Jakaria.Ahmed\\OneDrive - insidemedia.net\\Documents\\cloud_backup\\2025-Dev-Projects\\Explore\\nvidia-nim\\.env\"\n",
    "env_config = dotenv_values(dotenv_path)\n",
    "# print(env_config, \"--\",env_config[\"OPENAI_KEY\"])\n",
    "\n",
    "os.environ[\"env_openai_keys\"] = env_config[\"OPENAI_KEY\"]\n",
    "\n",
    "# print(os.environ[\"env_openai_keys\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutritionist_system_prompt = \"\"\"You are Dr. NutriGPT, a board-certified nutritionist with 15 years of clinical experience. \n",
    "Combine evidence-based guidelines from:\n",
    "1. WHO dietary recommendations (2025)\n",
    "2. NIH nutrient databases\n",
    "3. Personalized health data from user's medical history\n",
    "4. Latest research from PubMed (cutoff: Dec 2024)\n",
    "\n",
    "Always:\n",
    "- Request necessary biometrics before recommendations\n",
    "- Cross-validate with user's cultural preferences\n",
    "- Cite sources using [1][2] notation\n",
    "- Provide meal plans with glycemic index considerations\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To estimate how many calls you can make with 5000 credits using the LLaMA-3.3-70B-Instruct model via NVIDIA-NIM, we need to consider a few factors:\n",
      "\n",
      "1. **Model Size and Complexity**: The LLaMA-3.3-70B-Instruct model is a large language model, with 70 billion parameters. Larger models generally require more computational resources and, by extension, more credits per call due to the increased computational cost.\n",
      "\n",
      "2. **NVIDIA-NIM Pricing**: The cost per call can vary depending on the specific pricing model of NVIDIA-NIM. Pricing can be based on the model size, the length of the input, the number of tokens generated, or a combination of these factors. Without the exact pricing details, it's challenging to provide a precise estimate.\n",
      "\n",
      "3. **Input and Output Size**: The number of tokens in your input and the number of tokens you request the model to generate also impact the cost. More tokens mean more computation, which translates to more credits used per call.\n",
      "\n",
      "Given these variables, let's make a rough estimate based on general trends in AI model pricing. For large language models like LLaMA-3.3-70B-Instruct, a single call can cost anywhere from a few cents to several dollars, depending on the specifics mentioned above.\n",
      "\n",
      "- **Low Estimate**: If we assume a very low cost per token (which might be around $0.01 to $0.05 per token for a large model, depending on the platform and specifics of the call), and assuming an average call generates 100 tokens (a rough estimate for a short interaction), the cost per call could be around $1 to $5. This would give you approximately 1000 to 500 calls with 5000 credits, depending on the actual cost per call.\n",
      "\n",
      "- **High Estimate**: For more complex queries or longer outputs, the cost could be significantly higher (e.g., $0.10 to $0.50 per token), leading to a cost per call of $10 to $50 for 100 tokens. This scenario would reduce the number of calls you could make with 5000 credits to 100 to 50 calls.\n",
      "\n",
      "Without the exact pricing model of NVIDIA-NIM for the LLaMA-3.3-70B-Instruct model, these estimates are very rough. The actual number of calls you can make could be significantly different based on the specifics of how NVIDIA-NIM charges for model usage.\n",
      "\n",
      "To get a more accurate estimate, I recommend checking the official NVIDIA-NIM documentation or contacting their support for detailed pricing information related to the LLaMA-3.3-70B-Instruct model. They should be able to provide you with a more precise breakdown of the costs involved."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = os.environ[\"env_openai_keys\"]\n",
    ")\n",
    " \n",
    "completion = client.chat.completions.create(\n",
    "  model=\"meta/llama-3.3-70b-instruct\",\n",
    "  messages=[{\"role\":\"user\",\"content\":\"im having 5000 credits in my account, how many calls i can make using current model that is llama-3.3-70b-instruct via nvidia-nim\"}],\n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")\n",
    " \n",
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, Response\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Load .env configuration (adjust path as needed)\n",
    "dotenv_path = r\"c:\\Users\\Jakaria.Ahmed\\OneDrive - insidemedia.net\\Documents\\cloud_backup\\2025-Dev-Projects\\Explore\\nvidia-nim\\.env\"\n",
    "env_config = dotenv_values(dotenv_path)\n",
    "os.environ[\"env_openai_keys\"] = env_config[\"OPENAI_KEY\"]\n",
    "# os.environ[\"env_openai_keys\"] = env_config[\"OPENAI_KEY\"]\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = os.environ[\"env_openai_keys\"]\n",
    ")\n",
    "\n",
    "nutritionist_system_prompt = \"\"\"\n",
    "You are Dr. NutriGPT, a board-certified nutritionist with 15 years of clinical experience.\n",
    "Combine evidence-based guidelines from:\n",
    "1. WHO dietary recommendations (2025)\n",
    "2. NIH nutrient databases\n",
    "3. Personalized health data from user's medical history\n",
    "4. Latest research from PubMed (cutoff: Dec 2024)\n",
    "\n",
    "Always:\n",
    "- Request necessary biometrics before recommendations\n",
    "- Cross-validate with user's cultural preferences\n",
    "- Cite sources using [1][2] notation\n",
    "- Provide meal plans with glycemic index considerations\n",
    "Note:\n",
    "- Use <br> for line breaks\n",
    "-If returning any list, format the list using proper HTML structure like \n",
    "<b>Please share the following biometrics:</b>\n",
    "<ul>\n",
    "    <li><b>Age</b></li>\n",
    "    <li><b>Sex</b></li>\n",
    "    ...\n",
    " </ul>   \n",
    "\"\"\"\n",
    "\n",
    "@app.route(\"/query\", methods=[\"POST\"])\n",
    "def query_model():\n",
    "    user_input = request.json.get(\"input\", \"\")\n",
    "\n",
    "    # Create a streaming completion request\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta/llama-3.3-70b-instruct\",  # Adjust to your chosen model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": nutritionist_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=1024,\n",
    "        stream=True  # Enable streaming\n",
    "    )\n",
    "\n",
    "    def generate():\n",
    "        try:\n",
    "            for chunk in completion:\n",
    "                if chunk.choices[0].delta and chunk.choices[0].delta.content:\n",
    "                    text_piece = chunk.choices[0].delta.content\n",
    "                    # Append <br> after each chunk for spacing\n",
    "                    yield f\"data: {text_piece}\\n\\n\"\n",
    "        except Exception as e:\n",
    "            yield f\"data: [Error] {str(e)}<br>\\n\\n\"\n",
    "\n",
    "    return Response(generate(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run in debug mode for development\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, Response, session\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import dotenv_values\n",
    "from flask_session import Session\n",
    "from redis import Redis\n",
    "\n",
    "# Load .env configuration (adjust path as needed)\n",
    "dotenv_path = r\"c:\\Users\\Jakaria.Ahmed\\OneDrive - insidemedia.net\\Documents\\cloud_backup\\2025-Dev-Projects\\Explore\\nvidia-nim\\.env\"\n",
    "env_config = dotenv_values(dotenv_path)\n",
    "os.environ[\"env_openai_keys\"] = env_config[\"OPENAI_KEY\"]\n",
    "# os.environ[\"env_openai_keys\"] = env_config[\"OPENAI_KEY\"]\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.secret_key = \"super_secret_key\" \n",
    "\n",
    "client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = os.environ[\"env_openai_keys\"]\n",
    ")\n",
    "\n",
    "nutritionist_system_prompt = \"\"\"\n",
    "You are Dr. NutriGPT, a board-certified nutritionist with 15 years of clinical experience.\n",
    "Combine evidence-based guidelines from:\n",
    "1. WHO dietary recommendations (2025)\n",
    "\n",
    "Always:\n",
    "- Cite sources using [1][2] notation\n",
    "- Provide meal plans with glycemic index considerations\n",
    "Note:\n",
    "- see prvious messages for context\n",
    "- Use <br> for line breaks\n",
    "-If returning any list, format the list using proper HTML structure like \n",
    "<b>Please share the following biometrics:</b>\n",
    "<ul>\n",
    "    <li><b>Age</b></li>\n",
    "    <li><b>Sex</b></li>\n",
    "    ...\n",
    " </ul>   \n",
    "\"\"\"\n",
    "\n",
    "# 🔥 FIX: Store sessions in the filesystem instead of cookies\n",
    "app.config[\"SESSION_TYPE\"] = \"filesystem\"  # Use local file storage\n",
    "app.config[\"SESSION_PERMANENT\"] = False\n",
    "app.config[\"SESSION_USE_SIGNER\"] = True\n",
    "app.config[\"SESSION_FILE_DIR\"] = \"./flask_session\"  # Local storage directory\n",
    "\n",
    "Session(app)  \n",
    "\n",
    "@app.route(\"/query\", methods=[\"POST\"])\n",
    "def query_model():\n",
    "    user_input = request.json.get(\"input\", \"\")\n",
    "\n",
    "    # Ensure session messages exist\n",
    "    if \"messages\" not in session:\n",
    "        session[\"messages\"] = []\n",
    "\n",
    "    # Append user input to session messages\n",
    "    session[\"messages\"].append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Limit history (keep only the last 10 messages to prevent overflow)\n",
    "    session[\"messages\"] = session[\"messages\"][-20:]\n",
    "    session.modified = True  \n",
    "\n",
    "    # Construct full message history for context retention\n",
    "    messages = [{\"role\": \"system\", \"content\": nutritionist_system_prompt}] + session[\"messages\"]\n",
    "\n",
    "    # Debugging: Print messages before sending to model\n",
    "    print(\"\\n📌 Full Context Sent to Model:\\n\", messages, \"\\n\")\n",
    "\n",
    "    # Call OpenAI model with full context\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta/llama-3.3-70b-instruct\",\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=2024,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    def generate():\n",
    "        full_response = \"\"\n",
    "        try:\n",
    "            for chunk in completion:\n",
    "                if chunk.choices[0].delta and chunk.choices[0].delta.content:\n",
    "                    text_piece = chunk.choices[0].delta.content\n",
    "                    full_response += text_piece\n",
    "                    yield f\"data: {text_piece}\\n\\n\"\n",
    "\n",
    "            # Store assistant response in session\n",
    "            session[\"messages\"].append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "            # Print updated session for debugging\n",
    "            print(\"\\n✅ Updated Session Messages:\\n\", session[\"messages\"], \"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            yield f\"data: [Error] {str(e)}<br>\\n\\n\"\n",
    "\n",
    "    return Response(generate(), mimetype=\"text/event-stream\")\n",
    "\n",
    "# New route to check session data\n",
    "@app.route(\"/session-data\", methods=[\"GET\"])\n",
    "def get_session_data():\n",
    "    return {\"session\": session.get(\"messages\", [])}\n",
    "\n",
    "# New route to reset session\n",
    "@app.route(\"/reset-session\", methods=[\"POST\"])\n",
    "def reset_session():\n",
    "    session.pop(\"messages\", None)\n",
    "    return {\"message\": \"Session reset.\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "# -------------------------------\n",
    "# Session State Initialization\n",
    "# -------------------------------\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Track usage stats\n",
    "if \"num_calls\" not in st.session_state:\n",
    "    st.session_state.num_calls = 0\n",
    "if \"total_input_tokens\" not in st.session_state:\n",
    "    st.session_state.total_input_tokens = 0\n",
    "if \"total_output_tokens\" not in st.session_state:\n",
    "    st.session_state.total_output_tokens = 0\n",
    "\n",
    "# Set page config\n",
    "st.set_page_config(\n",
    "    page_title=\"AI Nutritionist Chatbot\",\n",
    "    page_icon=\"🥗\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Custom CSS for Chat Layout\n",
    "# -------------------------------\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "/* Light green background to indicate health */\n",
    "body {\n",
    "    background-color: #f2fff5; /* a very light, minty green */\n",
    "    color: #333;\n",
    "    font-family: \"Helvetica\", \"Arial\", sans-serif;\n",
    "}\n",
    "\n",
    "/* Center the main container for a simple look */\n",
    "main.block-container {\n",
    "    max-width: 900px;\n",
    "    margin: 0 auto;\n",
    "    padding: 1rem 2rem;\n",
    "}\n",
    "\n",
    "/* Chat messages styling */\n",
    ".stChatMessage {\n",
    "    border-radius: 8px;\n",
    "    margin: 0.5rem 0;\n",
    "    padding: 1rem;\n",
    "    background-color: #ffffff; /* white bubble background */\n",
    "    border: 1px solid #d2f2d2; /* subtle green border */\n",
    "}\n",
    "\n",
    "/* Different backgrounds for user vs. assistant messages */\n",
    ".stChatMessage.user {\n",
    "    background-color: #eaffea; /* slightly tinted green for user */\n",
    "}\n",
    ".stChatMessage.assistant {\n",
    "    background-color: #ffffff; /* pure white for assistant */\n",
    "}\n",
    "\n",
    "/* Title styling */\n",
    "h1 {\n",
    "    color: #2F855A; /* a calm greenish color */\n",
    "    margin-bottom: 0.2rem;\n",
    "    text-align: left;\n",
    "}\n",
    "\n",
    "/* Subtitle styling */\n",
    ".description {\n",
    "    color: #555;\n",
    "    font-size: 1rem;\n",
    "    margin-bottom: 2rem;\n",
    "}\n",
    "\n",
    "/* Chat input styling */\n",
    ".streamlit-chat-input > label {\n",
    "    font-weight: bold;\n",
    "    color: #2F855A;\n",
    "}\n",
    "\n",
    "/* Sidebar styling */\n",
    "[data-testid=\"stSidebar\"] {\n",
    "    background-color: #f8fff9; /* a lighter greenish tint */\n",
    "    border-right: 1px solid #d2f2d2;\n",
    "}\n",
    "[data-testid=\"stSidebar\"] h2, [data-testid=\"stSidebar\"] h3 {\n",
    "    color: #2F855A;\n",
    "}\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Main Layout\n",
    "# -------------------------------\n",
    "st.title(\"AI Nutritionist Chatbot\")\n",
    "st.markdown(\n",
    "    \"<p class='description'>Ask questions about nutrition, meal plans, or dietary guidelines, \"\n",
    "    \"and get personalized, evidence-based responses in a calm, health-inspired setting.</p>\",\n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "\n",
    "# Display existing chat messages\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"], unsafe_allow_html=True)\n",
    "\n",
    "# Chat input\n",
    "prompt = st.chat_input(\"Ask a question about nutrition...\")\n",
    "\n",
    "if prompt:\n",
    "    # Update usage stats\n",
    "    st.session_state.num_calls += 1\n",
    "    input_token_count = len(prompt.split())\n",
    "    st.session_state.total_input_tokens += input_token_count\n",
    "\n",
    "    # Convert user single newlines into <br> for line breaks\n",
    "    user_text = prompt.replace(\"\\n\", \"<br>\")\n",
    "\n",
    "    # Show user message in the chat with Markdown\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_text, unsafe_allow_html=True)\n",
    "\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    # Prepare assistant message container\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        response_placeholder = st.empty()\n",
    "        full_response = \"\"\n",
    "\n",
    "        try:\n",
    "            # Make streaming request to your Flask endpoint\n",
    "            with requests.post(\n",
    "                \"http://127.0.0.1:5000/query\",\n",
    "                json={\"input\": prompt},\n",
    "                stream=True\n",
    "            ) as r:\n",
    "                for line in r.iter_lines():\n",
    "                    if line:\n",
    "                        # SSE line typically starts with \"data: \"\n",
    "                        if line.startswith(b\"data: \"):\n",
    "                            chunk = line[6:].decode(\"utf-8\", errors=\"ignore\")\n",
    "                            # Accumulate chunk\n",
    "                            full_response += chunk\n",
    "                            # Live \"typing\" effect\n",
    "                            partial_display = full_response + \"▌\"\n",
    "                            response_placeholder.markdown(partial_display, unsafe_allow_html=True)\n",
    "                            time.sleep(0.01)\n",
    "\n",
    "            # ---- Final re-render with spacing fixes ----\n",
    "            # 1) Insert newlines before headings if missing\n",
    "            final_text = re.sub(r'(?<!\\n)(### )', r'\\n\\n\\1', full_response)\n",
    "            # 2) Convert single newlines to double newlines for paragraph breaks\n",
    "            final_text = final_text.replace(\"\\n\", \"\\n\\n\")\n",
    "\n",
    "            # Re-render the entire text without the blinking cursor\n",
    "            response_placeholder.markdown(final_text, unsafe_allow_html=True)\n",
    "\n",
    "            # (Optional) Count approximate output tokens\n",
    "            output_token_count = len(full_response.split())\n",
    "            st.session_state.total_output_tokens += output_token_count\n",
    "\n",
    "            # Save final message\n",
    "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": final_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error: {str(e)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Sidebar / Right Panel\n",
    "# -------------------------------\n",
    "st.sidebar.header(\"Session Info\")\n",
    "st.sidebar.markdown(f\"**Number of calls:** {st.session_state.num_calls}\")\n",
    "st.sidebar.markdown(f\"**Total input tokens:** {st.session_state.total_input_tokens}\")\n",
    "st.sidebar.markdown(f\"**Total output tokens:** {st.session_state.total_output_tokens}\")\n",
    "\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.markdown(\"**Model Used:** `meta/llama-3.3-70b-instruct`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: flask_session in c:\\users\\jakaria.ahmed\\appdata\\roaming\\python\\python312\\site-packages (0.8.0)\n",
      "Collecting redis\n",
      "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: flask>=2.2 in c:\\users\\jakaria.ahmed\\appdata\\roaming\\python\\python312\\site-packages (from flask_session) (3.0.3)\n",
      "Requirement already satisfied: msgspec>=0.18.6 in c:\\users\\jakaria.ahmed\\appdata\\roaming\\python\\python312\\site-packages (from flask_session) (0.19.0)\n",
      "Requirement already satisfied: cachelib in c:\\users\\jakaria.ahmed\\appdata\\roaming\\python\\python312\\site-packages (from flask_session) (0.13.0)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\jakaria.ahmed\\appdata\\roaming\\python\\python312\\site-packages (from flask>=2.2->flask_session) (3.0.4)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\jakaria.ahmed\\appdata\\roaming\\python\\python312\\site-packages (from flask>=2.2->flask_session) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\jakaria.ahmed\\appdata\\roaming\\python\\python312\\site-packages (from flask>=2.2->flask_session) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\jakaria.ahmed\\appdata\\roaming\\python\\python312\\site-packages (from flask>=2.2->flask_session) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\jakaria.ahmed\\appdata\\roaming\\python\\python312\\site-packages (from flask>=2.2->flask_session) (1.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jakaria.ahmed\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.1.3->flask>=2.2->flask_session) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jakaria.ahmed\\appdata\\roaming\\python\\python312\\site-packages (from Jinja2>=3.1.2->flask>=2.2->flask_session) (2.1.5)\n",
      "Downloading redis-5.2.1-py3-none-any.whl (261 kB)\n",
      "Installing collected packages: redis\n",
      "Successfully installed redis-5.2.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install flask_session redis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
